#!/usr/bin/env python3
# -*- coding: utf-8
"""
create_stardict_compatible_db.py - åˆ›å»ºä¸ç°æœ‰appå…¼å®¹çš„StarDictæ ¼å¼æ•°æ®åº“

è¿™ä¸ªè„šæœ¬å°†chinese-dictionaryçš„æ•°æ®è½¬æ¢ä¸ºä¸ç°æœ‰appå…¼å®¹çš„æ ¼å¼ï¼š
1. åˆ›å»ºSQLiteæ•°æ®åº“ï¼ŒåŒ…å«wordIndexè¡¨ï¼ˆid, word, offset, lengthï¼‰
2. ç”ŸæˆStarDictæ ¼å¼çš„dictæ–‡ä»¶
3. ç”ŸæˆStarDictæ ¼å¼çš„idxæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python create_stardict_compatible_db.py --db chinese_dictionary.db --output output_dir

è¾“å‡ºæ–‡ä»¶:
    - chinese_dict.db: ä¸ç°æœ‰appå…¼å®¹çš„SQLiteæ•°æ®åº“
    - chinese_dict.dict: StarDictæ ¼å¼çš„å­—å…¸å†…å®¹æ–‡ä»¶
    - chinese_dict.idx: StarDictæ ¼å¼çš„ç´¢å¼•æ–‡ä»¶
    - chinese_dict.ifo: StarDictæ ¼å¼çš„ä¿¡æ¯æ–‡ä»¶
"""

import sys
import sqlite3
import struct
import json
import argparse
from pathlib import Path
from typing import List, Tuple, Dict, Any


class StarDictCompatibleBuilder:
    """åˆ›å»ºä¸ç°æœ‰appå…¼å®¹çš„StarDictæ ¼å¼æ•°æ®åº“"""
    
    def __init__(self, source_db: str, output_dir: str):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            source_db: æºæ•°æ®åº“è·¯å¾„ï¼ˆchinese_dictionary.dbï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.source_db = Path(source_db)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.dict_db_path = self.output_dir / "chinese_dict.db"
        self.dict_file_path = self.output_dir / "chinese_dict.dictcontent"
        self.idx_file_path = self.output_dir / "chinese_dict.idx"
        self.ifo_file_path = self.output_dir / "chinese_dict.ifo"
        
    def create_database_schema(self):
        """åˆ›å»ºä¸ç°æœ‰appå…¼å®¹çš„æ•°æ®åº“è¡¨ç»“æ„"""
        print("åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...")
        
        conn = sqlite3.connect(self.dict_db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºwordIndexè¡¨ï¼Œä¸createDictSqliteDbIndex.pyä¸­çš„ç»“æ„ä¸€è‡´
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS wordIndex(
                `id` INTEGER PRIMARY KEY,
                `word` VARCHAR(128),
                `offset` INTEGER,
                `length` INTEGER,
                CONSTRAINT `word` UNIQUE (`word`)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_word ON wordIndex(word)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_offset ON wordIndex(offset)')
        
        conn.commit()
        conn.close()
        
        print(f"æ•°æ®åº“è¡¨ç»“æ„åˆ›å»ºå®Œæˆ: {self.dict_db_path}")
        
    def format_word_content(self, word_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è¯è¯­å†…å®¹ï¼Œæå–æœ‰ä»·å€¼çš„ä¿¡æ¯
        
        Args:
            word_data: è¯è¯­æ•°æ®å­—å…¸
            
        Returns:
            str: æ ¼å¼åŒ–çš„HTMLå†…å®¹
        """
        word = word_data.get('word', '')
        pinyin = word_data.get('pinyin', '')
        explanation = word_data.get('explanation', '')
        abbr = word_data.get('abbr', '')
        
        # æ„å»ºHTMLå†…å®¹
        html_parts = []
        
        # æ ‡é¢˜è¡Œ
        if pinyin:
            html_parts.append(f'<h2>{word} [{pinyin}]</h2>')
        else:
            html_parts.append(f'<h2>{word}</h2>')
        
        # ç¼©å†™
        if abbr:
            html_parts.append(f'<p><strong>ç¼©å†™:</strong> {abbr}</p>')
        
        # è§£é‡Š
        if explanation:
            html_parts.append(f'<p><strong>è§£é‡Š:</strong> {explanation}</p>')
        
        # å…¶ä»–å­—æ®µ
        fields = [
            ('source', 'æ¥æº'),
            ('quote', 'å¼•ç”¨'),
            ('story', 'å…¸æ•…'),
            ('similar', 'è¿‘ä¹‰è¯'),
            ('opposite', 'åä¹‰è¯'),
            ('example', 'ä¾‹å¥'),
            ('usage', 'ç”¨æ³•'),
            ('notice', 'æ³¨æ„')
        ]
        
        for field, label in fields:
            value = word_data.get(field)
            if value:
                if isinstance(value, (list, dict)):
                    value = json.dumps(value, ensure_ascii=False)
                html_parts.append(f'<p><strong>{label}:</strong> {value}</p>')
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œæä¾›é»˜è®¤ä¿¡æ¯
        if len(html_parts) <= 1:
            html_parts.append('<p>æš‚æ— è¯¦ç»†è§£é‡Š</p>')
        
        return '\n'.join(html_parts)
        
    def format_character_content(self, char_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ±‰å­—å†…å®¹ï¼Œæå–æœ‰ä»·å€¼çš„ä¿¡æ¯
        
        Args:
            char_data: æ±‰å­—æ•°æ®å­—å…¸
            
        Returns:
            str: æ ¼å¼åŒ–çš„HTMLå†…å®¹
        """
        char = char_data.get('char', '')
        pinyin = char_data.get('pinyin', [])
        strokes = char_data.get('strokes', 0)
        radicals = char_data.get('radicals', '')
        frequency = char_data.get('frequency', 0)
        structure = char_data.get('structure', '')
        traditional = char_data.get('traditional', '')
        variant = char_data.get('variant', '')
        explanations = char_data.get('explanations', [])
        
        # æ„å»ºHTMLå†…å®¹
        html_parts = []
        
        # æ ‡é¢˜è¡Œ
        if pinyin:
            pinyin_str = ', '.join(pinyin) if isinstance(pinyin, list) else pinyin
            html_parts.append(f'<h2>{char} [{pinyin_str}]</h2>')
        else:
            html_parts.append(f'<h2>{char}</h2>')
        
        # åŸºæœ¬ä¿¡æ¯
        if strokes > 0:
            html_parts.append(f'<p><strong>ç¬”ç”»:</strong> {strokes}ç”»</p>')
        
        if radicals:
            html_parts.append(f'<p><strong>éƒ¨é¦–:</strong> {radicals}</p>')
        
        if structure:
            html_parts.append(f'<p><strong>ç»“æ„:</strong> {structure}</p>')
        
        if frequency > 0:
            html_parts.append(f'<p><strong>ä½¿ç”¨é¢‘ç‡:</strong> {frequency}</p>')
        
        # ç¹ä½“å­—
        if traditional and traditional != char:
            html_parts.append(f'<p><strong>ç¹ä½“:</strong> {traditional}</p>')
        
        # å¼‚ä½“å­—
        if variant and variant != char:
            html_parts.append(f'<p><strong>å¼‚ä½“:</strong> {variant}</p>')
        
        # è¯¦ç»†è§£é‡Š
        if explanations and isinstance(explanations, list):
            html_parts.append('<p><strong>è¯¦ç»†è§£é‡Š:</strong></p>')
            for i, explanation in enumerate(explanations, 1):
                if isinstance(explanation, dict):
                    # å¤„ç†å­—å…¸æ ¼å¼çš„è§£é‡Š
                    meaning = explanation.get('meaning', '')
                    examples = explanation.get('examples', [])
                    
                    if meaning:
                        html_parts.append(f'<p>{i}. {meaning}</p>')
                    
                    if examples and isinstance(examples, list):
                        for example in examples:
                            html_parts.append(f'<p style="margin-left: 20px; color: #666;">ä¾‹: {example}</p>')
                else:
                    # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è§£é‡Š
                    html_parts.append(f'<p>{i}. {explanation}</p>')
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œæä¾›é»˜è®¤ä¿¡æ¯
        if len(html_parts) <= 1:
            html_parts.append('<p>æš‚æ— è¯¦ç»†è§£é‡Š</p>')
        
        return '\n'.join(html_parts)
        
    def format_merged_content(self, merged_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–åˆå¹¶åçš„æ•°æ®å†…å®¹
        
        Args:
            merged_data: åˆå¹¶åçš„æ•°æ®å­—å…¸
            
        Returns:
            str: æ ¼å¼åŒ–çš„HTMLå†…å®¹
        """
        word = merged_data.get('word', '')
        char_pinyin = merged_data.get('char_pinyin', '')
        char_explanation = merged_data.get('char_explanation', '')
        char_radicals = merged_data.get('char_radicals', '')
        char_strokes = merged_data.get('char_strokes', 0)
        char_frequency = merged_data.get('char_frequency', 0)
        
        word_pinyin = merged_data.get('word_pinyin', '')
        word_abbr = merged_data.get('word_abbr', '')
        word_explanation = merged_data.get('word_explanation', '')
        word_source = merged_data.get('word_source', '')
        word_quote = merged_data.get('word_quote', '')
        word_story = merged_data.get('word_story', '')
        word_similar = merged_data.get('word_similar', '')
        word_opposite = merged_data.get('word_opposite', '')
        word_example = merged_data.get('word_example', '')
        word_usage = merged_data.get('word_usage', '')
        word_notice = merged_data.get('word_notice', '')
        word_type = merged_data.get('word_type', '')
        
        html_parts = []
        
        # æ ‡é¢˜è¡Œ
        if char_pinyin:
            html_parts.append(f'<h2>{word} [{char_pinyin}]</h2>')
        else:
            html_parts.append(f'<h2>{word}</h2>')
        
        # æ±‰å­—ä¿¡æ¯
        if char_radicals:
            html_parts.append(f'<p><strong>éƒ¨é¦–:</strong> {char_radicals}</p>')
        if char_strokes > 0:
            html_parts.append(f'<p><strong>ç¬”ç”»:</strong> {char_strokes}ç”»</p>')
        if char_frequency > 0:
            html_parts.append(f'<p><strong>ä½¿ç”¨é¢‘ç‡:</strong> {char_frequency}</p>')
        
        # è¯è¯­ä¿¡æ¯
        if word_pinyin:
            html_parts.append(f'<p><strong>æ‹¼éŸ³:</strong> {word_pinyin}</p>')
        if word_abbr:
            html_parts.append(f'<p><strong>ç¼©å†™:</strong> {word_abbr}</p>')
        if word_explanation:
            html_parts.append(f'<p><strong>è§£é‡Š:</strong> {word_explanation}</p>')
        
        # å…¶ä»–å­—æ®µ
        fields = [
            ('word_source', 'æ¥æº'),
            ('word_quote', 'å¼•ç”¨'),
            ('word_story', 'å…¸æ•…'),
            ('word_similar', 'è¿‘ä¹‰è¯'),
            ('word_opposite', 'åä¹‰è¯'),
            ('word_example', 'ä¾‹å¥'),
            ('word_usage', 'ç”¨æ³•'),
            ('word_notice', 'æ³¨æ„')
        ]
        
        for field, label in fields:
            value = merged_data.get(field)
            if value:
                if isinstance(value, (list, dict)):
                    value = json.dumps(value, ensure_ascii=False)
                html_parts.append(f'<p><strong>{label}:</strong> {value}</p>')
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œæä¾›é»˜è®¤ä¿¡æ¯
        if len(html_parts) <= 1:
            html_parts.append('<p>æš‚æ— è¯¦ç»†è§£é‡Š</p>')
        
        return '\n'.join(html_parts)
        
    def process_data(self):
        """å¤„ç†æ•°æ®å¹¶ç”ŸæˆStarDictæ ¼å¼æ–‡ä»¶"""
        print("å¼€å§‹å¤„ç†æ•°æ®...")
        
        # è¿æ¥æ•°æ®åº“
        source_conn = sqlite3.connect(self.source_db)
        source_cursor = source_conn.cursor()
        
        target_conn = sqlite3.connect(self.dict_db_path)
        target_cursor = target_conn.cursor()
        
        # æ‰“å¼€æ–‡ä»¶
        dict_file = open(self.dict_file_path, 'wb')
        idx_file = open(self.idx_file_path, 'wb')
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ•°æ®å¹¶åˆå¹¶
            print("ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å’Œåˆå¹¶æ•°æ®...")
            
            # æ”¶é›†æ±‰å­—æ•°æ®
            source_cursor.execute('''
                SELECT char, pinyin, explanations, radicals, strokes, frequency
                FROM characters
                ORDER BY char
            ''')
            characters = source_cursor.fetchall()
            print(f"æ‰¾åˆ° {len(characters)} ä¸ªæ±‰å­—")
            
            # æ”¶é›†è¯è¯­æ•°æ®
            source_cursor.execute('''
                SELECT word, pinyin, abbr, explanation, source, quote, story,
                       similar, opposite, example, usage, notice, word_type
                FROM words
                ORDER BY word
            ''')
            words = source_cursor.fetchall()
            print(f"æ‰¾åˆ° {len(words)} ä¸ªè¯è¯­")
            
            # åˆå¹¶æ•°æ®ï¼šä½¿ç”¨å­—å…¸æ¥é¿å…é‡å¤
            merged_data = {}
            
            # å¤„ç†æ±‰å­—æ•°æ®
            for char_data in characters:
                char = char_data[0]
                char_dict = {
                    'type': 'character',
                    'char': char,
                    'pinyin': char_data[1],
                    'explanation': char_data[2],
                    'radicals': char_data[3],
                    'strokes': char_data[4],
                    'frequency': char_data[5]
                }
                merged_data[char] = char_dict
            
            # å¤„ç†è¯è¯­æ•°æ®ï¼Œå¦‚æœä¸æ±‰å­—é‡å¤åˆ™åˆå¹¶
            for word_data in words:
                word = word_data[0]
                word_dict = {
                    'type': 'word',
                    'word': word,
                    'pinyin': word_data[1],
                    'abbr': word_data[2],
                    'explanation': word_data[3],
                    'source': word_data[4],
                    'quote': word_data[5],
                    'story': word_data[6],
                    'similar': word_data[7],
                    'opposite': word_data[8],
                    'example': word_data[9],
                    'usage': word_data[10],
                    'notice': word_data[11],
                    'word_type': word_data[12]
                }
                
                if word in merged_data:
                    # å¦‚æœè¯è¯­ä¸æ±‰å­—é‡å¤ï¼Œåˆå¹¶æ•°æ®
                    existing = merged_data[word]
                    if existing['type'] == 'character':
                        # å°†æ±‰å­—æ•°æ®è½¬æ¢ä¸ºåˆå¹¶æ•°æ®
                        merged_dict = {
                            'type': 'merged',
                            'word': word,
                            'char_pinyin': existing['pinyin'],
                            'char_explanation': existing['explanation'],
                            'char_radicals': existing['radicals'],
                            'char_strokes': existing['strokes'],
                            'char_frequency': existing['frequency'],
                            'word_pinyin': word_dict['pinyin'],
                            'word_abbr': word_dict['abbr'],
                            'word_explanation': word_dict['explanation'],
                            'word_source': word_dict['source'],
                            'word_quote': word_dict['quote'],
                            'word_story': word_dict['story'],
                            'word_similar': word_dict['similar'],
                            'word_opposite': word_dict['opposite'],
                            'word_example': word_dict['example'],
                            'word_usage': word_dict['usage'],
                            'word_notice': word_dict['notice'],
                            'word_type': word_dict['word_type']
                        }
                        merged_data[word] = merged_dict
                        print(f"åˆå¹¶æ•°æ®: {word} (æ±‰å­—+è¯è¯­)")
                else:
                    # å¦‚æœè¯è¯­ä¸é‡å¤ï¼Œç›´æ¥æ·»åŠ 
                    merged_data[word] = word_dict
            
            print(f"åˆå¹¶åå…±æœ‰ {len(merged_data)} ä¸ªæ¡ç›®")
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰é¡ºåºå¤„ç†åˆå¹¶åçš„æ•°æ®
            print("ç¬¬äºŒæ­¥ï¼šå¤„ç†åˆå¹¶åçš„æ•°æ®...")
            
            current_offset = 0
            total_processed = 0
            
            # æŒ‰å­—ç¬¦é¡ºåºæ’åº
            sorted_items = sorted(merged_data.items(), key=lambda x: x[0])
            
            for word, data in sorted_items:
                # æ ¹æ®æ•°æ®ç±»å‹æ ¼å¼åŒ–å†…å®¹
                if data['type'] == 'character':
                    content = self.format_character_content(data)
                elif data['type'] == 'word':
                    content = self.format_word_content(data)
                elif data['type'] == 'merged':
                    content = self.format_merged_content(data)
                else:
                    continue
                
                content_bytes = content.encode('utf-8')
                content_length = len(content_bytes)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šå¯¹äº"æœ­è®°"è¿™ä¸ªè¯
                if word == 'æœ­è®°':
                    print(f"ğŸ” è°ƒè¯•æœ­è®°:")
                    print(f"   æ•°æ®ç±»å‹: {data['type']}")
                    print(f"   å½“å‰åç§»é‡: {current_offset}")
                    print(f"   å†…å®¹é•¿åº¦: {content_length}")
                    print(f"   æ ¼å¼åŒ–å†…å®¹: {content[:100]}...")
                
                # å†™å…¥dictæ–‡ä»¶
                dict_file.write(content_bytes)
                
                # å†™å…¥idxæ–‡ä»¶ï¼šè¯è¯­ + nullç»ˆæ­¢ç¬¦ + åç§»é‡(4å­—èŠ‚) + é•¿åº¦(4å­—èŠ‚)
                word_bytes = word.encode('utf-8')
                idx_file.write(word_bytes)
                idx_file.write(b'\0')  # nullç»ˆæ­¢ç¬¦
                idx_file.write(struct.pack('>I', current_offset))  # åç§»é‡ï¼Œå¤§ç«¯åº
                idx_file.write(struct.pack('>I', content_length))  # é•¿åº¦ï¼Œå¤§ç«¯åº
                
                # æ’å…¥æ•°æ®åº“ï¼ˆç»Ÿä¸€è½¬å°å†™ï¼‰
                target_cursor.execute('''
                    INSERT INTO wordIndex(word, offset, length)
                    VALUES (?, ?, ?)
                ''', (word.lower(), current_offset, content_length))
                
                # è°ƒè¯•ä¿¡æ¯ï¼šå¯¹äº"æœ­è®°"è¿™ä¸ªè¯
                if word == 'æœ­è®°':
                    print(f"   æ•°æ®åº“æ’å…¥æˆåŠŸ: {word.lower()}, {current_offset}, {content_length}")
                
                # æ›´æ–°åç§»é‡
                current_offset += content_length
                total_processed += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if total_processed % 1000 == 0:
                    print(f"å¤„ç†è¿›åº¦: {total_processed}/{len(sorted_items)} ({total_processed/len(sorted_items)*100:.1f}%)")
            
            print(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_processed} ä¸ªæ¡ç›®")
            
            # æäº¤æ•°æ®åº“æ›´æ”¹
            target_conn.commit()
            
        finally:
            dict_file.close()
            idx_file.close()
            source_conn.close()
            target_conn.close()
        
    def create_ifo_file(self):
        """åˆ›å»ºStarDictæ ¼å¼çš„.ifoä¿¡æ¯æ–‡ä»¶"""
        print("åˆ›å»º.ifoä¿¡æ¯æ–‡ä»¶...")
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        dict_size = self.dict_file_path.stat().st_size
        idx_size = self.idx_file_path.stat().st_size
        
        # è®¡ç®—è¯è¯­æ•°é‡ï¼ˆæ¯ä¸ªæ¡ç›®ï¼šè¯è¯­ + null + 4å­—èŠ‚åç§»é‡ + 4å­—èŠ‚é•¿åº¦ï¼‰
        # éœ€è¦ä»idxæ–‡ä»¶è®¡ç®—å®é™…æ¡ç›®æ•°
        conn = sqlite3.connect(self.dict_db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM wordIndex')
        word_count = cursor.fetchone()[0]
        conn.close()
        
        # åˆ›å»º.ifoæ–‡ä»¶å†…å®¹
        ifo_content = f"""StarDict's dict ifo file
version=2.4.2
bookname=Chinese Dictionary
wordcount={word_count}
idxfilesize={idx_size}
sametypesequence=h
"""
        
        with open(self.ifo_file_path, 'w', encoding='utf-8') as f:
            f.write(ifo_content)
        
        print(f".ifoæ–‡ä»¶åˆ›å»ºå®Œæˆ: {self.ifo_file_path}")
        
    def build(self):
        """æ‰§è¡Œå®Œæ•´çš„æ„å»ºæµç¨‹"""
        print("å¼€å§‹æ„å»ºStarDictå…¼å®¹çš„æ•°æ®åº“...")
        
        try:
            # 1. åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„
            self.create_database_schema()
            
            # 2. å¤„ç†æ•°æ®å¹¶ç”Ÿæˆæ–‡ä»¶
            self.process_data()
            
            # 3. åˆ›å»º.ifoæ–‡ä»¶
            self.create_ifo_file()
            
            print("\nğŸ‰ æ„å»ºå®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ğŸ—„ï¸  æ•°æ®åº“: {self.dict_db_path}")
            print(f"ğŸ“– å­—å…¸æ–‡ä»¶: {self.dict_file_path}")
            print(f"ğŸ” ç´¢å¼•æ–‡ä»¶: {self.idx_file_path}")
            print(f"â„¹ï¸  ä¿¡æ¯æ–‡ä»¶: {self.ifo_file_path}")
            
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            print(f"\nğŸ“Š æ–‡ä»¶å¤§å°:")
            print(f"   .db: {self.dict_db_path.stat().st_size / 1024 / 1024:.2f} MB")
            print(f"   .dict: {self.dict_file_path.stat().st_size / 1024 / 1024:.2f} MB")
            print(f"   .idx: {self.idx_file_path.stat().st_size / 1024:.2f} KB")
            print(f"   .ifo: {self.ifo_file_path.stat().st_size} bytes")
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åˆ›å»ºä¸ç°æœ‰appå…¼å®¹çš„StarDictæ ¼å¼æ•°æ®åº“')
    parser.add_argument('--db', required=True, help='æºæ•°æ®åº“è·¯å¾„ï¼ˆchinese_dictionary.dbï¼‰')
    parser.add_argument('--output', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æºæ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if not Path(args.db).exists():
        print(f"âŒ æºæ•°æ®åº“ä¸å­˜åœ¨: {args.db}")
        sys.exit(1)
    
    # åˆ›å»ºæ„å»ºå™¨å¹¶æ‰§è¡Œæ„å»º
    builder = StarDictCompatibleBuilder(args.db, args.output)
    success = builder.build()
    
    if success:
        print("\nâœ… ç°åœ¨æ‚¨å¯ä»¥å°†ç”Ÿæˆçš„æ–‡ä»¶ç”¨äºç°æœ‰çš„appäº†ï¼")
        print("   æ•°æ®åº“æ–‡ä»¶å¯ä»¥ç›´æ¥æ›¿æ¢ç°æœ‰çš„æ•°æ®åº“æ–‡ä»¶")
        print("   .dict, .idx, .ifoæ–‡ä»¶å¯ä»¥ç”¨äºStarDictå…¼å®¹çš„å­—å…¸è½¯ä»¶")
    else:
        print("\nâŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == '__main__':
    main()
