#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­—å…¸æ„é€ ç¨‹åº - æ±‰è¯­æ‹¼éŸ³è¾å…¸

å®ç°åˆ†å±‚æ„é€ ç­–ç•¥ï¼š
1. åŸºç¡€æ±‰å­—å­—å…¸ï¼ˆå¸¸ç”¨å­— -> é€šç”¨è§„èŒƒæ±‰å­— -> å…¨éƒ¨æ±‰å­—ï¼‰
2. è¯è¯­å­—å…¸ï¼ˆæˆè¯­ -> å¸¸ç”¨è¯è¯­ -> å…¨éƒ¨è¯è¯­ï¼‰
3. åˆå¹¶ä¼˜åŒ–å’ŒStarDictç”Ÿæˆ
"""

import os
import sys
import json
import sqlite3
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import argparse


@dataclass
class Character:
    """æ±‰å­—æ•°æ®ç»“æ„"""
    char: str
    pinyin: List[str]
    strokes: int
    radicals: str
    frequency: int
    structure: str
    traditional: Optional[str] = None
    variant: Optional[str] = None
    explanations: Optional[List[Dict]] = None


@dataclass
class Word:
    """è¯è¯­æ•°æ®ç»“æ„"""
    word: str
    pinyin: str
    abbr: str
    explanation: str
    source: Optional[Dict] = None
    quote: Optional[Dict] = None
    story: Optional[List[str]] = None
    similar: Optional[List[str]] = None
    opposite: Optional[List[str]] = None
    example: Optional[str] = None
    usage: Optional[str] = None
    notice: Optional[str] = None


class DictionaryBuilder:
    """å­—å…¸æ„é€ å™¨"""
    
    def __init__(self, data_dir: str, output_dir: str):
        """
        åˆå§‹åŒ–å­—å…¸æ„é€ å™¨
        
        Args:
            data_dir: æ•°æ®æºç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.db_path = self.output_dir / "chinese_dictionary.db"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = self.output_dir / "dict_builder.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        self.logger.info("åˆå§‹åŒ–æ•°æ®åº“...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæ±‰å­—è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS characters (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                char TEXT UNIQUE NOT NULL,
                pinyin TEXT NOT NULL,
                strokes INTEGER,
                radicals TEXT,
                frequency INTEGER,
                structure TEXT,
                traditional TEXT,
                variant TEXT,
                explanations TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºè¯è¯­è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS words (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT UNIQUE NOT NULL,
                pinyin TEXT NOT NULL,
                abbr TEXT NOT NULL,
                explanation TEXT,
                source TEXT,
                quote TEXT,
                story TEXT,
                similar TEXT,
                opposite TEXT,
                example TEXT,
                usage TEXT,
                notice TEXT,
                word_type TEXT DEFAULT 'word',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_char_frequency ON characters(frequency)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_char_pinyin ON characters(pinyin)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_pinyin ON words(pinyin)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_type ON words(word_type)')
        
        conn.commit()
        conn.close()
        
        self.logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
        
    def _fix_json_format(self, content: str) -> str:
        """
        ä¿®å¤JSONæ ¼å¼é—®é¢˜
        
        Args:
            content: åŸå§‹JSONå†…å®¹
            
        Returns:
            str: ä¿®å¤åçš„JSONå†…å®¹
        """
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
        content = content.strip()
        
        # å¦‚æœå¼€å¤´ä¸æ˜¯ [ï¼Œæ·»åŠ  [
        if not content.startswith('['):
            content = '[' + content
        
        # å¦‚æœç»“å°¾ä¸æ˜¯ ]ï¼Œä¿®å¤ç»“å°¾
        if not content.endswith(']'):
            # ç§»é™¤æœ€åä¸€ä¸ªé€—å·å’Œæ¢è¡Œç¬¦ï¼Œç„¶åæ·»åŠ  ]
            content = content.rstrip().rstrip(',').rstrip() + '\n]'
        
        return content
        
    def load_character_data(self) -> List[Character]:
        """åŠ è½½æ±‰å­—æ•°æ®"""
        self.logger.info("åŠ è½½æ±‰å­—æ•°æ®...")
        
        characters = []
        
        # åŠ è½½åŸºç¡€æ±‰å­—æ•°æ®
        char_base_path = self.data_dir / "character" / "char_base.json"
        if char_base_path.exists():
            with open(char_base_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®å¤JSONæ ¼å¼é—®é¢˜
            content = self._fix_json_format(content)
            char_base_data = json.loads(content)
                
            for item in char_base_data:
                char = Character(
                    char=item['char'],
                    pinyin=item['pinyin'],
                    strokes=item.get('strokes', 0),
                    radicals=item.get('radicals', ''),
                    frequency=item.get('frequency', 5),
                    structure=item.get('structure', ''),
                    traditional=item.get('traditional'),
                    variant=item.get('variant')
                )
                characters.append(char)
                
            self.logger.info(f"åŠ è½½åŸºç¡€æ±‰å­—æ•°æ®: {len(characters)} ä¸ª")
        
        # åŠ è½½è¯¦ç»†è§£é‡Šæ•°æ®
        char_detail_path = self.data_dir / "character" / "char_detail.json"
        if char_detail_path.exists():
            with open(char_detail_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®å¤JSONæ ¼å¼é—®é¢˜
            content = self._fix_json_format(content)
            char_detail_data = json.loads(content)
                
            # åˆ›å»ºå­—ç¬¦åˆ°è§£é‡Šçš„æ˜ å°„
            detail_map = {item['char']: item for item in char_detail_data}
            
            # åˆå¹¶è§£é‡Šæ•°æ®
            for char in characters:
                if char.char in detail_map:
                    char.explanations = detail_map[char.char].get('pronunciations', [])
                    
            self.logger.info(f"åŠ è½½æ±‰å­—è¯¦ç»†è§£é‡Šæ•°æ®: {len(detail_map)} ä¸ª")
        
        return characters
    
    def load_word_data(self) -> List[Word]:
        """åŠ è½½è¯è¯­æ•°æ®"""
        self.logger.info("åŠ è½½è¯è¯­æ•°æ®...")
        
        words = []
        
        # åŠ è½½æˆè¯­æ•°æ®
        idiom_path = self.data_dir / "idiom" / "idiom.json"
        if idiom_path.exists():
            with open(idiom_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®å¤JSONæ ¼å¼é—®é¢˜
            content = self._fix_json_format(content)
            idiom_data = json.loads(content)
                
            for item in idiom_data:
                word = Word(
                    word=item['word'],
                    pinyin=item['pinyin'],
                    abbr=item['abbr'],
                    explanation=item.get('explanation', ''),
                    source=item.get('source'),
                    quote=item.get('quote'),
                    story=item.get('story'),
                    similar=item.get('similar'),
                    opposite=item.get('opposite'),
                    example=item.get('example'),
                    usage=item.get('usage'),
                    notice=item.get('notice')
                )
                words.append(word)
                
            self.logger.info(f"åŠ è½½æˆè¯­æ•°æ®: {len(words)} ä¸ª")
        
        # åŠ è½½è¯è¯­æ•°æ®
        word_path = self.data_dir / "word" / "word.json"
        if word_path.exists():
            with open(word_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®å¤JSONæ ¼å¼é—®é¢˜
            content = self._fix_json_format(content)
            word_data = json.loads(content)
                
            for item in word_data:
                # è·³è¿‡å·²ç»åœ¨æˆè¯­ä¸­çš„è¯
                if not any(w.word == item['word'] for w in words):
                    # å¤„ç†ç¼ºå°‘çš„å­—æ®µï¼Œæä¾›é»˜è®¤å€¼
                    word = Word(
                        word=item['word'],
                        pinyin=item['pinyin'],
                        abbr=item.get('abbr', ''),  # å¦‚æœæ²¡æœ‰abbrå­—æ®µï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                        explanation=item.get('explanation', ''),
                        source=item.get('source'),
                        quote=item.get('quote'),
                        story=item.get('story'),
                        similar=item.get('similar'),
                        opposite=item.get('opposite'),
                        example=item.get('example'),
                        usage=item.get('usage'),
                        notice=item.get('notice')
                    )
                    words.append(word)
                    
            self.logger.info(f"åŠ è½½è¯è¯­æ•°æ®: {len(words)} ä¸ª")
        
        return words
    
    def build_character_dictionary(self, characters: List[Character]):
        """æ„å»ºæ±‰å­—å­—å…¸"""
        self.logger.info("æ„å»ºæ±‰å­—å­—å…¸...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŒ‰é¢‘ç‡åˆ†ç»„å¤„ç†
        frequency_groups = {}
        for char in characters:
            freq = char.frequency
            if freq not in frequency_groups:
                frequency_groups[freq] = []
            frequency_groups[freq].append(char)
        
        # æŒ‰é¢‘ç‡é¡ºåºå¤„ç†ï¼ˆ0=æœ€å¸¸ç”¨ï¼Œ1=è¾ƒå¸¸ç”¨ï¼Œ2=æ¬¡å¸¸ç”¨ï¼Œ3=äºŒçº§å­—ï¼Œ4=ä¸‰çº§å­—ï¼Œ5=ç”Ÿåƒ»å­—ï¼‰
        for freq in sorted(frequency_groups.keys()):
            chars = frequency_groups[freq]
            self.logger.info(f"å¤„ç†é¢‘ç‡ {freq} çš„æ±‰å­—: {len(chars)} ä¸ª")
            
            for char in chars:
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO characters 
                        (char, pinyin, strokes, radicals, frequency, structure, 
                         traditional, variant, explanations)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        char.char,
                        json.dumps(char.pinyin, ensure_ascii=False),
                        char.strokes,
                        char.radicals,
                        char.frequency,
                        char.structure,
                        char.traditional,
                        char.variant,
                        json.dumps(char.explanations, ensure_ascii=False) if char.explanations else None
                    ))
                except Exception as e:
                    self.logger.error(f"æ’å…¥æ±‰å­—å¤±è´¥ {char.char}: {e}")
        
        conn.commit()
        conn.close()
        
        self.logger.info("æ±‰å­—å­—å…¸æ„å»ºå®Œæˆ")
    
    def build_word_dictionary(self, words: List[Word]):
        """æ„å»ºè¯è¯­å­—å…¸"""
        self.logger.info("æ„å»ºè¯è¯­å­—å…¸...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŒ‰ç±»å‹åˆ†ç»„å¤„ç†
        idiom_count = 0
        word_count = 0
        
        for word in words:
            try:
                # åˆ¤æ–­æ˜¯å¦ä¸ºæˆè¯­ï¼ˆé•¿åº¦>=4ä¸”åŒ…å«æˆè¯­ç‰¹å¾ï¼‰
                is_idiom = (len(word.word) >= 4 and 
                           (word.source or word.story or word.usage))
                
                word_type = 'idiom' if is_idiom else 'word'
                
                cursor.execute('''
                    INSERT OR REPLACE INTO words 
                    (word, pinyin, abbr, explanation, source, quote, story,
                     similar, opposite, example, usage, notice, word_type)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    word.word,
                    word.pinyin,
                    word.abbr,
                    word.explanation,
                    json.dumps(word.source, ensure_ascii=False) if word.source else None,
                    json.dumps(word.quote, ensure_ascii=False) if word.quote else None,
                    json.dumps(word.story, ensure_ascii=False) if word.story else None,
                    json.dumps(word.similar, ensure_ascii=False) if word.similar else None,
                    json.dumps(word.opposite, ensure_ascii=False) if word.opposite else None,
                    word.example,
                    word.usage,
                    word.notice,
                    word_type
                ))
                
                if is_idiom:
                    idiom_count += 1
                else:
                    word_count += 1
                    
            except Exception as e:
                self.logger.error(f"æ’å…¥è¯è¯­å¤±è´¥ {word.word}: {e}")
        
        conn.commit()
        conn.close()
        
        self.logger.info(f"è¯è¯­å­—å…¸æ„å»ºå®Œæˆ: æˆè¯­ {idiom_count} ä¸ªï¼Œè¯è¯­ {word_count} ä¸ª")
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ±‰å­—ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM characters')
        total_chars = cursor.fetchone()[0]
        
        cursor.execute('SELECT frequency, COUNT(*) FROM characters GROUP BY frequency ORDER BY frequency')
        char_freq_stats = cursor.fetchall()
        
        # è¯è¯­ç»Ÿè®¡
        cursor.execute('SELECT COUNT(*) FROM words')
        total_words = cursor.fetchone()[0]
        
        cursor.execute('SELECT word_type, COUNT(*) FROM words GROUP BY word_type')
        word_type_stats = cursor.fetchall()
        
        conn.close()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "statistics.txt"
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("æ±‰è¯­æ‹¼éŸ³è¾å…¸ - æ•°æ®ç»Ÿè®¡\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"æ±‰å­—æ€»æ•°: {total_chars}\n")
            f.write("æ±‰å­—é¢‘ç‡åˆ†å¸ƒ:\n")
            for freq, count in char_freq_stats:
                freq_names = {0: "æœ€å¸¸ç”¨", 1: "è¾ƒå¸¸ç”¨", 2: "æ¬¡å¸¸ç”¨", 
                             3: "äºŒçº§å­—", 4: "ä¸‰çº§å­—", 5: "ç”Ÿåƒ»å­—"}
                f.write(f"  {freq_names.get(freq, f'é¢‘ç‡{freq}')}: {count} ä¸ª\n")
            
            f.write(f"\nè¯è¯­æ€»æ•°: {total_words}\n")
            f.write("è¯è¯­ç±»å‹åˆ†å¸ƒ:\n")
            for word_type, count in word_type_stats:
                type_names = {'idiom': 'æˆè¯­', 'word': 'è¯è¯­'}
                f.write(f"  {type_names.get(word_type, word_type)}: {count} ä¸ª\n")
        
        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        return {
            'total_chars': total_chars,
            'char_freq_stats': char_freq_stats,
            'total_words': total_words,
            'word_type_stats': word_type_stats
        }
    
    def build(self):
        """æ‰§è¡Œå®Œæ•´çš„å­—å…¸æ„å»ºæµç¨‹"""
        try:
            self.logger.info("å¼€å§‹æ„å»ºæ±‰è¯­æ‹¼éŸ³è¾å…¸...")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºæ±‰å­—å­—å…¸
            self.logger.info("ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºæ±‰å­—å­—å…¸")
            characters = self.load_character_data()
            self.build_character_dictionary(characters)
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºè¯è¯­å­—å…¸
            self.logger.info("ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºè¯è¯­å­—å…¸")
            words = self.load_word_data()
            self.build_word_dictionary(words)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            self.logger.info("ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
            stats = self.generate_statistics()
            
            self.logger.info("å­—å…¸æ„å»ºå®Œæˆï¼")
            self.logger.info(f"æ±‰å­—æ€»æ•°: {stats['total_chars']}")
            self.logger.info(f"è¯è¯­æ€»æ•°: {stats['total_words']}")
            
        except Exception as e:
            self.logger.error(f"å­—å…¸æ„å»ºå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ±‰è¯­æ‹¼éŸ³è¾å…¸æ„å»ºç¨‹åº',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python dict_builder.py --data-dir chinese-dictionary-main --output-dir output
  
å‚æ•°è¯´æ˜:
  --data-dir: æ•°æ®æºç›®å½•è·¯å¾„ï¼ˆåŒ…å«characterã€wordã€idiomå­ç›®å½•ï¼‰
  --output-dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
    )
    
    parser.add_argument('--data-dir', required=True, help='æ•°æ®æºç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå­—å…¸æ„é€ å™¨å¹¶æ‰§è¡Œæ„å»º
        builder = DictionaryBuilder(args.data_dir, args.output_dir)
        builder.build()
        print(f"\nğŸ‰ å­—å…¸æ„å»ºå®Œæˆï¼")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"æ•°æ®åº“æ–‡ä»¶: {builder.db_path}")
        print(f"ç»Ÿè®¡ä¿¡æ¯: {args.output_dir}/statistics.txt")
        
    except Exception as e:
        print(f"\nâŒ å­—å…¸æ„å»ºå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
